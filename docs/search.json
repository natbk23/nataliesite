[
  {
    "objectID": "sql_audio.html",
    "href": "sql_audio.html",
    "title": "WAI Database SQL Query",
    "section": "",
    "text": "To replicate the table in Voss (2020), I will be pulling the measurements, PI info, and subject tables from the scidb.smith.edu database. After making sure I understand the data structure, I will turn the Frequency data to a log scale. I will then select and join the relevant data before preparing it for visualization. The above steps are in SQL, but I will use R’s ggplot package to produce the visualization.\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis shows us the different tables in the database.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nThis helped me understand the data structure of Measurements.\n\nSELECT * FROM PI_Info\nWHERE Identifier = \"Abur_2014\";\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nYear\nAuthors\nAuthorsShortList\nTitle\nJournal\nURL\nAbstract\nDataSubmitterName\nDataSubmitterEmail\nDateSubmitted\nPI_Notes\n\n\n\n\nAbur_2014\n2014\nDefne Abur, Nicholas J. Horton, and Susan E. Voss\nAbur et al.\nInstrasubject variability in power reflectance\nJ Am Acad Audiol\nhttps://www.ncbi.nlm.nih.gov/pubmed/25257718\n“\n\n\n\n\n\n\n\n\n Purpose:  This study investigates test-retest features of power reflectance, including comparisons of intrasubject versus intersubject variability and how ear-canal measurement location affects measurements.\n\n\n Research design:  Repeated measurements of power reflectance were made at about weekly intervals. The subjects returned for four to eight sessions. Measurements were made at three ear-canal locations: a deep insertion depth (with a foam plug flush at the entrance to the ear canal) and both 3 and 6 mm more lateral to this deep insertion.\n\n\n Study sample: Repeated measurements on seven subjects are reported. All subjects were female, between 19 and 22 yr old, and enrolled at an undergraduate women’s college.\n\n\n Data collection and analysis:  Measurements on both the right and left ears were made at three ear-canal locations during each of four to eight measurement sessions. Random-effects regression models were used for the analysis to account for repeated measures within subjects. The mean power reflectance for each position over all sessions was calculated for each subject.\n\n\n Results:  The comparison of power reflectance from the left and right ears of an individual subject varied greatly over the seven subjects; the difference between the power reflectance measured on the left and that measured on the right was compared at 248 frequencies, and depending on the subject, the percentage of tested frequencies for which the left and right ears differed significantly ranged from 10% to 93% (some with left values greater than right values and others with the opposite pattern). Although the individual subjects showed left-right differences, the overall population generally did not show significant differences between the left and right ears. The mean power reflectance for each measurement position over all sessions depended on the location of the probe in the ear for frequencies of less than 1000 Hz. The standard deviation between subjects’ mean power reflectance after controlling for ear (left or right) was found to be greater than the standard deviation within the individual subject’s mean power reflectance. The intrasubject standard deviation in power reflectance was smallest at the deepest insertion depths.\n\n\n Conclusions:  All subjects had differences in power reflectance between their left and right ears at some frequencies; the percentage of frequencies at which differences occurred varied greatly across subjects. The intrasubject standard deviations were smallest for the deepest probe insertion depths, suggesting clinical measurements should be made with as deep an insertion as practically possible to minimize variability. This deep insertion will reduce both acoustic leaks and the effect of low-frequency ear-canal losses. The within-subject standard deviations were about half the magnitude of the overall standard deviations, quantifying the extent of intrasubject versus intersubject variability.\n\n” |Susan Voss |svoss@smith.edu |24-Aug-2016 |Measurements made on 7 subjects across multiple sessions and 3 probe locations for each subject. Database includes measurements at only the deepest insertion depth (Position 1) and Channel B only. |\n\n\nI explored the PI_Info data by looking at a particular study using Identifier; Abur (2014).\n\nSELECT * FROM Measurements\nWHERE Identifier = \"Abur_2014\";\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n328.125\n0.0527801\n73326200\n-0.232837\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n351.562\n0.0583192\n68793600\n-0.232115\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n375.000\n0.0638881\n64088600\n-0.231642\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n398.438\n0.0687025\n60200600\n-0.231356\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n421.875\n0.0833181\n56990900\n-0.228356\n\n\n\n\n\nI then used the Identifier to explore the corresponding Measurements data.\n\nSELECT p.Identifier, Year, AuthorsShortList,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ears,\nCONCAT(AuthorsShortList, \" (\" , year, \") \" , \"N=\" ,\nCOUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS Legend\n\nFROM PI_Info AS p\n\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\n\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \n\"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \n\"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND \nFrequency &lt; 8000\n\nGROUP BY Identifier, Instrument;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nIdentifier\nYear\nAuthorsShortList\nears\nLegend\n\n\n\n\nAbur_2014\n2014\nAbur et al.\n14\nAbur et al. (2014) N=14; HearID\n\n\nFeeney_2017\n2017\nFeeney et al.\n57\nFeeney et al. (2017) N=57; preTitan\n\n\nGroon_2015\n2015\nGroon et al.\n21\nGroon et al. (2015) N=21; Other\n\n\nLewis_2015\n2015\nLewis and Neely\n14\nLewis and Neely (2015) N=14; Other\n\n\nLiu_2008\n2008\nLiu et al.\n92\nLiu et al. (2008) N=92; preTitan\n\n\nRosowski_2012\n2012\nRosowski et al.\n58\nRosowski et al. (2012) N=58; HearID\n\n\nShahnaz_2006\n2006\nShahnaz and Bork\n237\nShahnaz and Bork (2006) N=237; HearID\n\n\nShaver_2013\n2013\nShaver and Sun\n48\nShaver and Sun (2013) N=48; preTitan\n\n\nSun_2016\n2016\nSun\n84\nSun (2016) N=84; preTitan\n\n\nVoss_1994\n1994\nVoss and Allen\n10\nVoss and Allen (1994) N=10; preHearID\n\n\n\n\n\n\nSELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\n\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \n\"; \", Instrument) AS legend\n\nFROM PI_Info AS p\n\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\n\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \n\"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\" ) AND Frequency &gt; 200 AND Frequency &lt; 8000\n\nGROUP BY Identifier, Instrument, Frequency;\n\n\ngraph |&gt;\n  ggplot(aes (x= Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10() +\n  labs(\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    title = \"Mean Absorbance from Each Publication in WAI Database\",\n    color = \"Legend\")\n\n\n\n\n\n\n\n\nThis graph is created by joining Measurements with p.Identifier. There appears to be a strong positive correlation between Frequency and Mean Absorbance overall, but it differs between the studies.\nQuestion 2:\n\nSELECT *\nFROM Subjects\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n8\n5\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n10\n5\n19.0000000\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 4 not included do to acoustic leak\n\n\nAithal_2013\n1\n1\nNA\nInfant\nMale\nUnknown\nUnknown\nNormal\nUnknown\nNA\n\n\nAithal_2013\n2\n1\n0.0074418\nInfant\nFemale\nUnknown\nUnknown\nNormal\nUnknown\nNA\n\n\nAithal_2013\n3\n1\nNA\nInfant\nMale\nUnknown\nUnknown\nUnknown\nNormal\nNA\n\n\n\n\n\nI wanted to examine the Subjects dataframe to determine which studies had participants of diverse races.\n\nSELECT Frequency, Race,\n    AVG(Absorbance) AS ave_absorbance\nFROM Subjects AS s\nINNER JOIN Measurements AS m ON m.SubjectNumber = s.SubjectNumber\nWHERE m.Identifier = \"Rosowski_2012\" AND s.Identifier = \"Rosowski_2012\" \nAND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Race, Frequency;\n\n\nq2_graph |&gt;\n  ggplot(aes(x = Frequency, y = ave_absorbance, color = Race, group = Race)) +\n  geom_line() +\n  labs(\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    title = \"Mean Absorbance from Rosowski et al across Race\",\n    color = \"Race\"\n  ) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nInterestingly, Rosowski (2012) separated “Chinese” from “Asian” and had very little racial diversity overall. The graphs convey that after a strong positive correlation, average absorbance rapidly dropped after Frequency of 3000.\nFor Question 1, it was important that I aggregate data for specific studies with Identifier and that I filter within the frequency range of 80000 &gt; Frequency &gt; 200. For Question 2,I wanted to explore the race variable. After determining that Rosowski (2012) enrolled multiple race groups, I joined the relevant variables using the “Rosowksi_2012” Identifier. I then graphed the data, grouping by race.\nArticle Citations:\nVoss SE. Resource Review. Ear Hear. 2019 Nov/Dec;40(6):1481. doi: 10.1097/AUD.0000000000000790. PMID: 31651606; PMCID: PMC7093226.\nRosowski, J. J., Nakajima, H. H., Hamade, M. A., Mahfoud, L., Merchant, G. R., Halpin, C. F., & Merchant, S. N. (2012). Ear-canal reflectance, umbo velocity, and tympanometry in normal-hearing adults. Ear and Hearing, 33(1), 19–34. https://doi.org/10.1097/AUD.0b013e31822ccb76\nDatabase Source: https://www.science.smith.edu/wai-database/"
  },
  {
    "objectID": "HCDexperience.html",
    "href": "HCDexperience.html",
    "title": "Human-Centered Experience Design",
    "section": "",
    "text": "/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project 2.html",
    "href": "Project 2.html",
    "title": "Visualizing Netflix Data",
    "section": "",
    "text": "This project analyzes Netflix’s catalog of movies and TV shows using TidyTuesday data. Visualizations explore content ratings, genres, and global contributions, with a focus on Asian production compared to the US and UK. The findings highlight surprising trends, including the diversity of Netflix’s offerings and patterns in content distribution.\n\nlibrary(tidytuesdayR)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\nnetflix &lt;- tuesdata$netflix\n\nnetflix |&gt;\n  filter(!is.na(rating), type == \"Movie\") |&gt;\n  group_by(type, rating) |&gt;\n  summarize(count = n()) |&gt;\n  filter(count &gt; 0) |&gt;\n  ggplot(aes(x = rating, y = count)) + \n    geom_bar(stat = \"identity\") +\n    labs(x = \"Media Type\", y = \"Count\", title = \"Types of Media\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThese graphs convey the distribution of movies for each rating.\n\nnetflix |&gt;\n  mutate(clean_genres = str_remove_all(netflix$listed_in, \",.*\"))\n\n# A tibble: 7,787 × 13\n   show_id type    title director   cast  country date_added release_year rating\n   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 s1      TV Show 3%    &lt;NA&gt;       João… Brazil  August 14…         2020 TV-MA \n 2 s2      Movie   7:19  Jorge Mic… Demi… Mexico  December …         2016 TV-MA \n 3 s3      Movie   23:59 Gilbert C… Tedd… Singap… December …         2011 R     \n 4 s4      Movie   9     Shane Ack… Elij… United… November …         2009 PG-13 \n 5 s5      Movie   21    Robert Lu… Jim … United… January 1…         2008 PG-13 \n 6 s6      TV Show 46    Serdar Ak… Erda… Turkey  July 1, 2…         2016 TV-MA \n 7 s7      Movie   122   Yasir Al … Amin… Egypt   June 1, 2…         2019 TV-MA \n 8 s8      Movie   187   Kevin Rey… Samu… United… November …         1997 R     \n 9 s9      Movie   706   Shravan K… Divy… India   April 1, …         2019 TV-14 \n10 s10     Movie   1920  Vikram Bh… Rajn… India   December …         2008 TV-MA \n# ℹ 7,777 more rows\n# ℹ 4 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;,\n#   clean_genres &lt;chr&gt;\n\n\nThis function eliminates the cross-listing of genres to make for an easier analysis.\n\nr_comedies &lt;- netflix |&gt;\n  mutate(clean_genres = str_remove_all(listed_in, \",.*\")) |&gt;\n  filter(str_detect(clean_genres, \"Comedies\")) |&gt;\n  summarize(percent_r = mean(rating == \"R\") * 100)\nprint(r_comedies)\n\n# A tibble: 1 × 1\n  percent_r\n      &lt;dbl&gt;\n1      10.6\n\n\n10.6% of all comedies are Rated R- I predicted higher!\n\nnum_the &lt;- length(grep(\"^The \", netflix$title, value = TRUE))\nprint(num_the)\n\n[1] 982\n\n\nI was curious how many shows and movies start with “The”. I am surprised to find only 982!\n\nsum(str_detect(netflix$country, \"Singapore|China|India|South Korea|Indonesia|Japan|Taiwan|Philippines\") & !is.na(netflix$country))\n\n[1] 1877\n\n\nI was wondering how much Netflix content comes from Asian production. str_detect lets us know that 1877 of the shows and movies on Netflix are from Asia.\n\nasia &lt;- c(\"Singapore\", \"China\", \"India\", \"South Korea\", \"Indonesia\", \"Japan\", \n          \"Taiwan\", \"Philippines\")\n\nnetflix |&gt;\n  filter(country %in% c(asia, \"United States\", \"United Kingdom\")) |&gt;\n  mutate(country = ifelse(str_detect(country, paste(asia, collapse = \"|\")), \n                          \"Asia\", country)) |&gt;\n  group_by(type, country) |&gt;\n  summarize(count = n()) |&gt;\n  ggplot(aes(x = type, y = count, fill = country)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Movies and TV Shows from Asia, UK, and USA \")\n\n\n\n\n\n\n\n\nThis graph conveys the proportion of Asian, UK, and USA production in movies and TV shows. I am surprised at how much Asian production there is, and anticipated more British!\nSources:\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIt was found on the TidyTuesday github; https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20"
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Scandinavian World Heritage Sites",
    "section": "",
    "text": "This project examines data on UNESCO World Heritage Sites from TidyTuesday, focusing on temporal and geographical trends. The dataset includes information on heritage site values for various countries, spanning multiple years.The analysis compares site values in 2004 and 2022, revealing insights into changes in preservation efforts and heritage site recognition across nations.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-06')\nheritage &lt;- tuesdata$heritage\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tidyverse)\ndata_long &lt;- heritage %&gt;%\n     pivot_longer(cols = c(`2004`, `2022`), names_to = \"year\", values_to = \"value\")\nlibrary(ggplot2)\nggplot(data_long, aes(x = year, y = value, color = country, group = country)) +\n     geom_line(size = 1) +\n     geom_point(size = 3) +\n     labs(title = \"Country Values in 2004 and 2022\",\n          x = \"Year\",\n          y = \"Value\") +\n     scale_color_manual(values = c(\"Norway\" = \"blue\", \"Denmark\" = \"red\", \"Sweden\"\n                                   = \"green\")) +\n     theme_minimal()\n\n\n\n\n\n\n\n\nSources:\nData Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-02-06\nhttps://whc.unesco.org/en/list"
  },
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Refugees in Germany Over Time",
    "section": "",
    "text": "This analysis examines refugee and asylum seeker trends in Germany using TidyTuesday data. The visualizations track changes over time, highlighting the distinct patterns for each group. These findings provide insight into migration dynamics and Germany’s role in global displacement trends.\n\nlibrary(dplyr)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-08-22')\npopulation &lt;- tuesdata$population\ndesired_variables &lt;- population |&gt;\n     select(year, coa_name, refugees, asylum_seekers)\ngermany_data &lt;- desired_variables |&gt;\n     filter(coa_name == \"Germany\")\nsummary_germany &lt;- germany_data |&gt;\n     group_by(year) |&gt;\n     summarize(\n         total_refugees = sum(refugees, na.rm = TRUE),\n         total_asylum_seekers = sum(asylum_seekers, na.rm = TRUE),\n         .groups = 'drop'\n    )\n\nlibrary(ggplot2)\nggplot(summary_germany, aes(x = year)) +\n     geom_line(aes(y = total_refugees, color = \"Refugees\"), size = 1) +\n     geom_line(aes(y = total_asylum_seekers, color = \"Asylum Seekers\"), size = 1) +\n     labs(\n         title = \"Total Refugees and Asylum Seekers in Germany by Year\",\n         x = \"Year\",\n         y = \"Count\",\n         color = \"Legend\"\n     ) +\n     theme_minimal()\n\n\n\n\n\n\n\n\nSources:\nData Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-08-22\n\nData from UNHCR’s annual statistical activities dating back to 1951.\nData from the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA), specifically for registered Palestine refugees under UNRWA’s mandate.\nData from the Internal Displacement Monitoring Centre (IDMC) on people displaced within their country due to conflict or violence"
  },
  {
    "objectID": "clientpresentation.html#project-summary",
    "href": "clientpresentation.html#project-summary",
    "title": "Final Client Presentation",
    "section": "Project Summary",
    "text": "Project Summary\n\nFind a single study from the WAI database where subjects of different sex, race, ethnicity, or age groups were enrolled."
  },
  {
    "objectID": "clientpresentation.html#research-interest",
    "href": "clientpresentation.html#research-interest",
    "title": "Final Client Presentation",
    "section": "Research Interest",
    "text": "Research Interest\nI was particularly interested about differences in mean absorbance between race. However, exploring the Subjects dataframe taught me that very few studies were examining racial demographics."
  },
  {
    "objectID": "clientpresentation.html#findings",
    "href": "clientpresentation.html#findings",
    "title": "Final Client Presentation",
    "section": "Findings",
    "text": "Findings\nRosowski et al. (2012) is a study that took a unique approach to race.\n\nSELECT Frequency, Race,\n    AVG(Absorbance) AS ave_absorbance\nFROM Subjects AS s\nINNER JOIN Measurements AS m ON m.SubjectNumber = s.SubjectNumber\nWHERE m.Identifier = \"Rosowski_2012\" AND s.Identifier = \"Rosowski_2012\" \nAND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Race, Frequency;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSELECT Race, COUNT(*) AS count\nFROM Subjects\nWHERE Identifier = \"Rosowski_2012\"\nGROUP BY Race;\n\n\n4 records\n\n\nRace\ncount\n\n\n\n\nCaucasian\n20\n\n\nBlack\n6\n\n\nChinese\n2\n\n\nAsian\n1"
  },
  {
    "objectID": "clientpresentation.html#implications",
    "href": "clientpresentation.html#implications",
    "title": "Final Client Presentation",
    "section": "Implications",
    "text": "Implications\nRosowski et al.’s findings suggest that there were differences in average absorbance across “races”. However, the races are incredibly limited, and isolate Chinese from Asian.\n\nWhat does this mean for conclusions regarding race?\nHow do we even classify race?\n\n\nResearch indicates that the prevalence of hearing loss does differ across races, with studies consistently showing that non-Hispanic Black individuals generally have lower rates of hearing loss compared to non-Hispanic White individuals; meaning deafness may appear less frequently in the Black population compared to the White population.\n\nHowever, the data to support these claims is insufficient. Are black individuals truly less likely to experience hearing loss, or it just underreported?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Natalie Roston",
    "section": "",
    "text": "I am passionate about creating user-centric experiences and products that solve real-world problems. With a focus on empathy, I have worked on a variety of collaborative projects that have allowed me to continuously expand the boundaries of my heuristics. I invite you to explore some of my work in the “projects” tab and see how I have applied my skills to create impactful designs."
  },
  {
    "objectID": "permutationtest.html",
    "href": "permutationtest.html",
    "title": "Permutation Test on Box Office Success Between Genres",
    "section": "",
    "text": "Introduction\nIn this analysis, I wanted to test whether there is a significant difference in the average box office revenue between action and comedy movies using the `movies` dataset from OpenIntro. I use a permutation test to evaluate if any observed difference in average box office gross is statistically significant under the null hypothesis that genre does not affect revenue. I hypothesize that action movies have a higher box office revenue than comedies.\nData Setup\n\nlibrary(openintro)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(dplyr)\n\ndata(\"movies\")\n\nstr(movies)\n\nspc_tbl_ [140 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ movie     : chr [1:140] \"2Fast2Furious\" \"28DaysLater\" \"AGuyThing\" \"AManApart\" ...\n $ genre     : chr [1:140] \"action\" \"horror\" \"rom-comedy\" \"action\" ...\n $ score     : num [1:140] 48.9 78.2 39.5 42.9 79.9 57.9 35.1 50.7 62.6 63.3 ...\n $ rating    : chr [1:140] \"PG-13\" \"R\" \"PG-13\" \"R\" ...\n $ box_office: num [1:140] 127.1 45.1 15.5 26.2 17.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   movie = col_character(),\n  ..   genre = col_character(),\n  ..   score = col_double(),\n  ..   rating = col_character(),\n  ..   box.office = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThe above code sets up the data and allows me to understand its structure.\n\nmovies_clean &lt;- movies |&gt;\n  filter(!is.na(box_office)) \n\n\nobserved_diff &lt;- movies_clean |&gt;\n  group_by(genre) |&gt;\n  summarise(mean_box_office = mean(box_office, na.rm = TRUE)) |&gt;\n  filter(genre %in% c(\"action\", \"comedy\")) |&gt;\n  summarize(observed_diff = \n    mean_box_office[genre == \"action\"] - mean_box_office[genre == \"comedy\"]\n  )\n\nobserved_diff\n\n# A tibble: 1 × 1\n  observed_diff\n          &lt;dbl&gt;\n1          31.3\n\n\n\npermute_diff &lt;- function(data) {\n  data |&gt;\n    mutate(shuffled_genre = sample(genre)) |&gt;\n    summarize(\n      observed_diff = mean(box_office[shuffled_genre == \"action\"], na.rm = TRUE) - \n                      mean(box_office[shuffled_genre == \"comedy\"], na.rm = TRUE)\n    ) |&gt;\n    pull(observed_diff) \n}\n\n\nobserved_diff &lt;- as.numeric(observed_diff)\n\nset.seed(4747) \nnum_simulations &lt;- 10000\nperm_diffs &lt;- map_dbl(1:num_simulations, ~ permute_diff(movies))\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(observed_diff))\np_value\n\n[1] 0.0386\n\n\n\nperm_data &lt;- data.frame(perm_diffs = perm_diffs)\n\nggplot(perm_data, aes(x = perm_diffs)) +\n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\", alpha = 0.7) + geom_vline(xintercept = observed_diff, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Permutation Test: Box Office Gross Differences Between Action and Comedy Movies\",\n    x = \"Difference in Mean Gross (Action - Comedy)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot illustrates the results of a permutation test designed to assess whether there is a statistically significant difference in box office revenue between action and comedy movies. In the histogram, we see the distribution of mean differences in box office revenue under the null hypothesis, which assumes there is no true effect of genre on box office performance. Each bar represents the frequency of mean differences obtained by randomly shuffling genre labels (action and comedy) across all movies, then calculating the difference in means for each shuffle. This simulated distribution of differences reveals the range of values we might expect if genre had no impact on revenue.\nThe red dashed line in the plot marks the observed difference in box office revenue between action and comedy movies as calculated from the original data set. This observed difference provides the basis for testing our hypothesis. If this red line lies well outside the central bulk of the histogram, it indicates that the observed difference is unusual compared to what we would expect under the null hypothesis. In other words, a position far from the central peak of the histogram suggests that the observed difference in revenue is not easily explainable by random variation alone.\n\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(observed_diff))\np_value\n\n[1] 0.0386\n\n\nThe position of the observed difference near the tail of the distribution aligns with a p-value of 0.0386, indicating a statistically significant result. This means that, under the null hypothesis, there is only a 3.86% chance of observing a difference as extreme as the one in the data set. Consequently, the plot supports the conclusion that there is likely a true difference in box office revenue between action and comedy movies, with genre possibly influencing financial performance. This visualization, therefore, provides both a statistical and a visual basis for concluding that genre may have an impact on box office success.\nThe permutation test and histogram assess whether the observed difference in box office revenue between action and comedy movies (31 million dollars) is statistically significant or likely due to random chance. The histogram shows the null distribution, representing what differences we might expect if genre had no impact. With a p-value of 0.0386, the observed difference is unlikely to have occurred by chance, suggesting a real difference in box office revenue between action and comedy movies in the broader population. This result supports the idea that action movies tend to gross more than comedies on average.\nSource:\nOpenIntro. (2021). movies dataset. In OpenIntro: Data sets for introductory statistics (Version 3.4). Retrieved from https://www.openintro.org"
  },
  {
    "objectID": "clientpresentation.html#takeaways",
    "href": "clientpresentation.html#takeaways",
    "title": "Final Client Presentation",
    "section": "Takeaways",
    "text": "Takeaways\nThis project reinforced the message that data isn’t everything."
  },
  {
    "objectID": "clientpresentation.html#thank-you",
    "href": "clientpresentation.html#thank-you",
    "title": "Final Client Presentation",
    "section": "Thank You!",
    "text": "Thank You!"
  },
  {
    "objectID": "paiproject.html#my-role",
    "href": "paiproject.html#my-role",
    "title": "Alumni Connection Figma Wireframe",
    "section": "My Role",
    "text": "My Role\nI acted as both Product Manager and UX Designer. After discussing with the engineers to fully understand our capabilities, I developed a comprehensive prototype on Figma and assisted in the creation of the site with React.js."
  },
  {
    "objectID": "2024hackathon.html#my-role",
    "href": "2024hackathon.html#my-role",
    "title": "Alcohol Safety Figma Wireframe",
    "section": "My Role",
    "text": "My Role\nI was the principal UX Designer, but also acted as product manager as I oversaw the entire development from ideation to presentation. We made sure to conduct user testing to maintain a human-centered approach throughout our iterations."
  },
  {
    "objectID": "Sparkathon24.html#my-role",
    "href": "Sparkathon24.html#my-role",
    "title": "Campus Rideshare Design for 2024 Sparkathon",
    "section": "My Role",
    "text": "My Role\nAs product manager and project lead, I oversaw the development of my team’s product. I started with initial user interviews to identify pain points, assisted in the design, and strategized our market and financial approach."
  }
]